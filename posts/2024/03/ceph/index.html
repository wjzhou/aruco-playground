<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><title>Ceph | wjzhou Personal Blog</title>
<meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="Ceph install Prepare the host sudo zypper in podman Install cephadm SUSE The suse&rsquo;s build-in ceph is out of date, so use the curl install instead
CEPH_RELEASE=18.2.0 # replace this with the active release curl --silent --remote-name --location https://download.ceph.com/rpm-${CEPH_RELEASE}/el9/noarch/cephadm chmod +x cephadm RHEL sudo dnf install --assumeyes centos-release-ceph-reef sudo dnf install --assumeyes cephadm Bootstrap the cluster #cephadm bootstrap --mon-ip *<mon-ip>* --cluster-network *<secondary ceph network>* sudo ./cephadm bootstrap --mon-ip 10.240.1.101 --cluster-network 10."><meta name=generator content="Hugo 0.124.1"><meta name=robots content="noindex, nofollow"><link rel=stylesheet href=/ananke/css/main.min.css><meta property="og:title" content="Ceph"><meta property="og:description" content="Ceph install Prepare the host sudo zypper in podman Install cephadm SUSE The suse&rsquo;s build-in ceph is out of date, so use the curl install instead
CEPH_RELEASE=18.2.0 # replace this with the active release curl --silent --remote-name --location https://download.ceph.com/rpm-${CEPH_RELEASE}/el9/noarch/cephadm chmod +x cephadm RHEL sudo dnf install --assumeyes centos-release-ceph-reef sudo dnf install --assumeyes cephadm Bootstrap the cluster #cephadm bootstrap --mon-ip *<mon-ip>* --cluster-network *<secondary ceph network>* sudo ./cephadm bootstrap --mon-ip 10.240.1.101 --cluster-network 10."><meta property="og:type" content="article"><meta property="og:url" content="https://blog.ellieiris.com/posts/2024/03/ceph/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-03-09T10:49:47-05:00"><meta property="article:modified_time" content="2024-03-09T10:49:47-05:00"><meta itemprop=name content="Ceph"><meta itemprop=description content="Ceph install Prepare the host sudo zypper in podman Install cephadm SUSE The suse&rsquo;s build-in ceph is out of date, so use the curl install instead
CEPH_RELEASE=18.2.0 # replace this with the active release curl --silent --remote-name --location https://download.ceph.com/rpm-${CEPH_RELEASE}/el9/noarch/cephadm chmod +x cephadm RHEL sudo dnf install --assumeyes centos-release-ceph-reef sudo dnf install --assumeyes cephadm Bootstrap the cluster #cephadm bootstrap --mon-ip *<mon-ip>* --cluster-network *<secondary ceph network>* sudo ./cephadm bootstrap --mon-ip 10.240.1.101 --cluster-network 10."><meta itemprop=datePublished content="2024-03-09T10:49:47-05:00"><meta itemprop=dateModified content="2024-03-09T10:49:47-05:00"><meta itemprop=wordCount content="587"><meta itemprop=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="Ceph"><meta name=twitter:description content="Ceph install Prepare the host sudo zypper in podman Install cephadm SUSE The suse&rsquo;s build-in ceph is out of date, so use the curl install instead
CEPH_RELEASE=18.2.0 # replace this with the active release curl --silent --remote-name --location https://download.ceph.com/rpm-${CEPH_RELEASE}/el9/noarch/cephadm chmod +x cephadm RHEL sudo dnf install --assumeyes centos-release-ceph-reef sudo dnf install --assumeyes cephadm Bootstrap the cluster #cephadm bootstrap --mon-ip *<mon-ip>* --cluster-network *<secondary ceph network>* sudo ./cephadm bootstrap --mon-ip 10.240.1.101 --cluster-network 10."></head><body class="ma0 avenir bg-near-white"><header><div class=bg-black><nav class="pv3 ph3 ph4-ns" role=navigation><div class="flex-l justify-between items-center center"><a href=/ class="f3 fw2 hover-white no-underline white-90 dib">wjzhou Personal Blog</a><div class="flex-l items-center"><div class=ananke-socials></div></div></div></nav></div></header><main class=pb7 role=main><article class="flex-l flex-wrap justify-between mw8 center ph3"><header class="mt4 w-100"><aside class="instapaper_ignoref b helvetica tracked">POSTS</aside><div id=sharing class="mt3 ananke-socials"></div><h1 class="f1 athelas mt3 mb1">Ceph</h1><time class="f6 mv4 dib tracked" datetime=2024-03-09T10:49:47-05:00>March 9, 2024</time></header><div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><h1 id=ceph-install>Ceph install</h1><h2 id=prepare-the-host>Prepare the host</h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>sudo zypper in podman
</span></span></code></pre></div><h2 id=install-cephadm>Install cephadm</h2><h3 id=suse>SUSE</h3><p>The suse&rsquo;s build-in ceph is out of date, so use the curl install instead</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>CEPH_RELEASE<span style=color:#f92672>=</span>18.2.0 <span style=color:#75715e># replace this with the active release</span>
</span></span><span style=display:flex><span>curl --silent --remote-name --location https://download.ceph.com/rpm-<span style=color:#e6db74>${</span>CEPH_RELEASE<span style=color:#e6db74>}</span>/el9/noarch/cephadm
</span></span><span style=display:flex><span>chmod +x cephadm
</span></span></code></pre></div><h3 id=rhel>RHEL</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>sudo dnf install --assumeyes centos-release-ceph-reef
</span></span><span style=display:flex><span>sudo dnf install --assumeyes cephadm
</span></span></code></pre></div><h2 id=bootstrap-the-cluster>Bootstrap the cluster</h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e>#cephadm bootstrap --mon-ip *&lt;mon-ip&gt;* --cluster-network *&lt;secondary ceph network&gt;*</span>
</span></span><span style=display:flex><span>sudo ./cephadm bootstrap --mon-ip 10.240.1.101 --cluster-network 10.241.1.0/24
</span></span></code></pre></div><h2 id=add-host>Add Host</h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>export NEW_CEPH_HOST<span style=color:#f92672>=</span>c2
</span></span><span style=display:flex><span>export NEW_CEPH_IP<span style=color:#f92672>=</span>10.240.1.102
</span></span><span style=display:flex><span>ssh-copy-id -f -i /etc/ceph/ceph.pub root@$NEW_CEPH_IP
</span></span><span style=display:flex><span><span style=color:#75715e>#ceph orch host add *&lt;newhost&gt;* [*&lt;ip&gt;*] [*&lt;label1&gt; ...*]</span>
</span></span><span style=display:flex><span>sudo ./cephadm shell -- ceph orch host add $NEW_CEPH_HOST $NEW_CEPH_IP _admin
</span></span></code></pre></div><h2 id=prepare-nvme-for-bluestore-db-and-wal>Prepare nvme for bluestore db and wal</h2><h3 id=wipe-disk-if-needed>wipe disk if needed</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># be very careful with the following command and only target the disk you need to clean</span>
</span></span><span style=display:flex><span>wipefs /dev/nvme0n1
</span></span><span style=display:flex><span>sudo wipefs --all /dev/nvme0n1
</span></span></code></pre></div><h3 id=create-lvm-lv-for-db>create lvm lv for db</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>sudo vgcreate vg-ceph-ssd /dev/nvme0n1
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># sudo lvremove -y /dev/vg-ceph-ssd/d01</span>
</span></span><span style=display:flex><span><span style=color:#75715e># sudo lvremove -y /dev/vg-ceph-ssd/d02</span>
</span></span><span style=display:flex><span><span style=color:#75715e># sudo lvremove -y /dev/vg-ceph-ssd/d03</span>
</span></span><span style=display:flex><span><span style=color:#75715e># sudo lvremove -y /dev/vg-ceph-ssd/d04</span>
</span></span><span style=display:flex><span><span style=color:#75715e># sudo lvremove -y /dev/vg-ceph-ssd/d05</span>
</span></span><span style=display:flex><span><span style=color:#75715e># sudo lvremove -y /dev/vg-ceph-ssd/d06</span>
</span></span><span style=display:flex><span><span style=color:#75715e># sudo lvremove -y /dev/vg-ceph-ssd/d07</span>
</span></span><span style=display:flex><span><span style=color:#75715e># sudo lvremove -y /dev/vg-ceph-ssd/d08</span>
</span></span><span style=display:flex><span><span style=color:#75715e># sudo lvremove -y /dev/vg-ceph-ssd/d09</span>
</span></span><span style=display:flex><span><span style=color:#75715e># sudo lvremove -y /dev/vg-ceph-ssd/d10</span>
</span></span><span style=display:flex><span><span style=color:#75715e># sudo lvremove -y /dev/vg-ceph-ssd/d11</span>
</span></span><span style=display:flex><span><span style=color:#75715e># sudo lvremove -y /dev/vg-ceph-ssd/d12</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>sudo lvcreate -L 100G -n d01 vg-ceph-ssd
</span></span><span style=display:flex><span>sudo lvcreate -L 100G -n d02 vg-ceph-ssd
</span></span><span style=display:flex><span>sudo lvcreate -L 100G -n d03 vg-ceph-ssd
</span></span><span style=display:flex><span>sudo lvcreate -L 100G -n d04 vg-ceph-ssd
</span></span><span style=display:flex><span>sudo lvcreate -L 100G -n d05 vg-ceph-ssd
</span></span><span style=display:flex><span>sudo lvcreate -L 100G -n d06 vg-ceph-ssd
</span></span><span style=display:flex><span>sudo lvcreate -L 100G -n d07 vg-ceph-ssd
</span></span><span style=display:flex><span>sudo lvcreate -L 100G -n d08 vg-ceph-ssd
</span></span><span style=display:flex><span>sudo lvcreate -L 100G -n d09 vg-ceph-ssd
</span></span><span style=display:flex><span>sudo lvcreate -L 100G -n d10 vg-ceph-ssd
</span></span><span style=display:flex><span>sudo lvcreate -L 100G -n d11 vg-ceph-ssd
</span></span><span style=display:flex><span>sudo lvcreate -L 100G -n d12 vg-ceph-ssd
</span></span></code></pre></div><h3 id=create-osds>create osds</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>sudo ./cephadm shell
</span></span><span style=display:flex><span>ceph orch daemon add osd c1:data_devices<span style=color:#f92672>=</span>/dev/sda,db_devices<span style=color:#f92672>=</span>/dev/vg-ceph-ssd/d01
</span></span><span style=display:flex><span>ceph orch daemon add osd c1:data_devices<span style=color:#f92672>=</span>/dev/sdb,db_devices<span style=color:#f92672>=</span>/dev/vg-ceph-ssd/d02
</span></span><span style=display:flex><span>ceph orch daemon add osd c1:data_devices<span style=color:#f92672>=</span>/dev/sdc,db_devices<span style=color:#f92672>=</span>/dev/vg-ceph-ssd/d03
</span></span><span style=display:flex><span>ceph orch daemon add osd c1:data_devices<span style=color:#f92672>=</span>/dev/sdd,db_devices<span style=color:#f92672>=</span>/dev/vg-ceph-ssd/d04
</span></span><span style=display:flex><span>ceph orch daemon add osd c1:data_devices<span style=color:#f92672>=</span>/dev/sde,db_devices<span style=color:#f92672>=</span>/dev/vg-ceph-ssd/d05
</span></span><span style=display:flex><span>ceph orch daemon add osd c1:data_devices<span style=color:#f92672>=</span>/dev/sdf,db_devices<span style=color:#f92672>=</span>/dev/vg-ceph-ssd/d06
</span></span><span style=display:flex><span>ceph orch daemon add osd c1:data_devices<span style=color:#f92672>=</span>/dev/sdg,db_devices<span style=color:#f92672>=</span>/dev/vg-ceph-ssd/d07
</span></span><span style=display:flex><span>ceph orch daemon add osd c1:data_devices<span style=color:#f92672>=</span>/dev/sdh,db_devices<span style=color:#f92672>=</span>/dev/vg-ceph-ssd/d08
</span></span><span style=display:flex><span>ceph orch daemon add osd c1:data_devices<span style=color:#f92672>=</span>/dev/sdi,db_devices<span style=color:#f92672>=</span>/dev/vg-ceph-ssd/d09
</span></span><span style=display:flex><span>ceph orch daemon add osd c1:data_devices<span style=color:#f92672>=</span>/dev/sdj,db_devices<span style=color:#f92672>=</span>/dev/vg-ceph-ssd/d10
</span></span><span style=display:flex><span>ceph orch daemon add osd c1:data_devices<span style=color:#f92672>=</span>/dev/sdk,db_devices<span style=color:#f92672>=</span>/dev/vg-ceph-ssd/d11
</span></span><span style=display:flex><span>ceph orch daemon add osd c1:data_devices<span style=color:#f92672>=</span>/dev/sdl,db_devices<span style=color:#f92672>=</span>/dev/vg-ceph-ssd/d12
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>ceph orch daemon add osd c2:data_devices<span style=color:#f92672>=</span>/dev/sda,db_devices<span style=color:#f92672>=</span>/dev/vg-ceph-ssd/d01
</span></span><span style=display:flex><span>ceph orch daemon add osd c2:data_devices<span style=color:#f92672>=</span>/dev/sdb,db_devices<span style=color:#f92672>=</span>/dev/vg-ceph-ssd/d02
</span></span><span style=display:flex><span>ceph orch daemon add osd c2:data_devices<span style=color:#f92672>=</span>/dev/sdc,db_devices<span style=color:#f92672>=</span>/dev/vg-ceph-ssd/d03
</span></span><span style=display:flex><span>ceph orch daemon add osd c2:data_devices<span style=color:#f92672>=</span>/dev/sdd,db_devices<span style=color:#f92672>=</span>/dev/vg-ceph-ssd/d04
</span></span><span style=display:flex><span>ceph orch daemon add osd c2:data_devices<span style=color:#f92672>=</span>/dev/sde,db_devices<span style=color:#f92672>=</span>/dev/vg-ceph-ssd/d05
</span></span><span style=display:flex><span>ceph orch daemon add osd c2:data_devices<span style=color:#f92672>=</span>/dev/sdf,db_devices<span style=color:#f92672>=</span>/dev/vg-ceph-ssd/d06
</span></span><span style=display:flex><span>ceph orch daemon add osd c2:data_devices<span style=color:#f92672>=</span>/dev/sdg,db_devices<span style=color:#f92672>=</span>/dev/vg-ceph-ssd/d07
</span></span><span style=display:flex><span>ceph orch daemon add osd c2:data_devices<span style=color:#f92672>=</span>/dev/sdh,db_devices<span style=color:#f92672>=</span>/dev/vg-ceph-ssd/d08
</span></span><span style=display:flex><span>ceph orch daemon add osd c2:data_devices<span style=color:#f92672>=</span>/dev/sdi,db_devices<span style=color:#f92672>=</span>/dev/vg-ceph-ssd/d09
</span></span><span style=display:flex><span>ceph orch daemon add osd c2:data_devices<span style=color:#f92672>=</span>/dev/sdj,db_devices<span style=color:#f92672>=</span>/dev/vg-ceph-ssd/d10
</span></span><span style=display:flex><span>ceph orch daemon add osd c2:data_devices<span style=color:#f92672>=</span>/dev/sdk,db_devices<span style=color:#f92672>=</span>/dev/vg-ceph-ssd/d11
</span></span><span style=display:flex><span>ceph orch daemon add osd c2:data_devices<span style=color:#f92672>=</span>/dev/sdl,db_devices<span style=color:#f92672>=</span>/dev/vg-ceph-ssd/d12
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>ceph orch daemon add osd c3:data_devices<span style=color:#f92672>=</span>/dev/sda,db_devices<span style=color:#f92672>=</span>/dev/vg-ceph-ssd/d01
</span></span><span style=display:flex><span>ceph orch daemon add osd c3:data_devices<span style=color:#f92672>=</span>/dev/sdb,db_devices<span style=color:#f92672>=</span>/dev/vg-ceph-ssd/d02
</span></span><span style=display:flex><span>ceph orch daemon add osd c3:data_devices<span style=color:#f92672>=</span>/dev/sdc,db_devices<span style=color:#f92672>=</span>/dev/vg-ceph-ssd/d03
</span></span><span style=display:flex><span>ceph orch daemon add osd c3:data_devices<span style=color:#f92672>=</span>/dev/sdd,db_devices<span style=color:#f92672>=</span>/dev/vg-ceph-ssd/d04
</span></span><span style=display:flex><span>ceph orch daemon add osd c3:data_devices<span style=color:#f92672>=</span>/dev/sde,db_devices<span style=color:#f92672>=</span>/dev/vg-ceph-ssd/d05
</span></span><span style=display:flex><span>ceph orch daemon add osd c3:data_devices<span style=color:#f92672>=</span>/dev/sdf,db_devices<span style=color:#f92672>=</span>/dev/vg-ceph-ssd/d06
</span></span><span style=display:flex><span>ceph orch daemon add osd c3:data_devices<span style=color:#f92672>=</span>/dev/sdg,db_devices<span style=color:#f92672>=</span>/dev/vg-ceph-ssd/d07
</span></span><span style=display:flex><span>ceph orch daemon add osd c3:data_devices<span style=color:#f92672>=</span>/dev/sdh,db_devices<span style=color:#f92672>=</span>/dev/vg-ceph-ssd/d08
</span></span><span style=display:flex><span>ceph orch daemon add osd c3:data_devices<span style=color:#f92672>=</span>/dev/sdi,db_devices<span style=color:#f92672>=</span>/dev/vg-ceph-ssd/d09
</span></span><span style=display:flex><span>ceph orch daemon add osd c3:data_devices<span style=color:#f92672>=</span>/dev/sdm,db_devices<span style=color:#f92672>=</span>/dev/vg-ceph-ssd/d10
</span></span><span style=display:flex><span>ceph orch daemon add osd c3:data_devices<span style=color:#f92672>=</span>/dev/sdk,db_devices<span style=color:#f92672>=</span>/dev/vg-ceph-ssd/d11
</span></span><span style=display:flex><span>ceph orch daemon add osd c3:data_devices<span style=color:#f92672>=</span>/dev/sdl,db_devices<span style=color:#f92672>=</span>/dev/vg-ceph-ssd/d12
</span></span></code></pre></div><h1 id=create-a-fast-osd-for-fast-pool>create a fast osd for fast pool</h1><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>sudo lvcreate -L 500G -n fast vg-ceph-ssd
</span></span></code></pre></div><h3 id=create-user>Create User</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>radosgw-admin user create --uid<span style=color:#f92672>=</span>wujun --display-name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Wujun Zhou&#34;</span> --email<span style=color:#f92672>=</span>w@ellieiris.com
</span></span></code></pre></div><h3 id=add-hdd-and-ssd-crush-rule-set>add hdd and ssd crush rule set</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ceph osd crush rule create-replicated ssd default host hdd
</span></span><span style=display:flex><span>ceph osd crush rule create-replicated ssd default host ssd
</span></span><span style=display:flex><span>ceph osd crush rule dump
</span></span><span style=display:flex><span>ceph config set osd osd_pool_default_crush_rule <span style=color:#ae81ff>1</span> <span style=color:#75715e># choose the default id</span>
</span></span></code></pre></div><h3 id=set-pool-rule-set>set pool rule set</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ceph osd lspools
</span></span><span style=display:flex><span><span style=color:#75715e>#ceph osd pool set POOL_NAME crush_rule RULENAME</span>
</span></span></code></pre></div><ul class=pa0></ul><div class="mt6 instapaper_ignoref"></div></div><aside class="w-30-l mt6-l"></aside></article></main><footer class="bg-black bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-between"><a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href=https://blog.ellieiris.com/>&copy; wjzhou Personal Blog 2024</a><div><div class=ananke-socials></div></div></div></footer></body></html>